<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Sunhanyu-Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Sunhanyu-Learning">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Sunhanyu-Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="sunhanyu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Sunhanyu-Learning" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Sunhanyu-Learning</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-推理加速/kvcache" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/kvcache/" class="article-date">
  <time class="dt-published" datetime="2025-07-31T03:10:49.000Z" itemprop="datePublished">2025-07-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">推理加速</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/kvcache/">kv-cache-源码解读</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-KVcache原理"><a href="#1-KVcache原理" class="headerlink" title="1.KVcache原理"></a>1.<strong>KVcache</strong>原理</h1><h1 id="2-kvcache-源码实现"><a href="#2-kvcache-源码实现" class="headerlink" title="2.kvcache 源码实现"></a>2.kvcache 源码实现</h1><p>init内实现两个全零的k和v的缓存矩阵，大小为max_batch_size x max_seq_len x n_local_kv_heads x head_dim。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.cache_v = torch.zeros(</span><br><span class="line">        (</span><br><span class="line">            args.max_batch_size,</span><br><span class="line">            args.max_seq_len,</span><br><span class="line">            <span class="variable language_">self</span>.n_local_kv_heads,</span><br><span class="line">            <span class="variable language_">self</span>.head_dim,</span><br><span class="line">        )</span><br><span class="line">    ).cuda()</span><br><span class="line"><span class="variable language_">self</span>.cache_k = torch.zeros(</span><br><span class="line">            (</span><br><span class="line">                args.max_batch_size,</span><br><span class="line">                args.max_seq_len,</span><br><span class="line">                <span class="variable language_">self</span>.n_local_kv_heads,</span><br><span class="line">                <span class="variable language_">self</span>.head_dim,</span><br><span class="line">            )</span><br><span class="line">        ).cuda()</span><br></pre></td></tr></table></figure>
<p>forward 函数中，将输入的 q, k, v 分别保存在 cache_q, cache_k, cache_v 中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">       self,</span></span><br><span class="line"><span class="params">       x: torch.Tensor,</span></span><br><span class="line"><span class="params">       start_pos: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">       freqs_cis: torch.Tensor,</span></span><br><span class="line"><span class="params">       mask: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">   </span>):</span><br><span class="line">   bsz, seqlen, _ = x.shape </span><br><span class="line">   xq, xk, xv = <span class="variable language_">self</span>.wq(x), <span class="variable language_">self</span>.wk(x), <span class="variable language_">self</span>.wv(x) </span><br><span class="line">   <span class="comment"># (bsz, seqlen, hidden_dim) 首先对输入的当前大小的数据进行特征空间的转换</span></span><br><span class="line"></span><br><span class="line">   xq = xq.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">   xk = xk.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">   xv = xv.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">   <span class="comment"># (bsz, seqlen, n_local_heads, head_dim) 数据根据head头reshpe成4维</span></span><br><span class="line"></span><br><span class="line">   xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)</span><br><span class="line">   <span class="comment">#添加旋转位置编码</span></span><br><span class="line"></span><br><span class="line">   <span class="variable language_">self</span>.cache_k = <span class="variable language_">self</span>.cache_k.to(xq)</span><br><span class="line">   <span class="variable language_">self</span>.cache_v = <span class="variable language_">self</span>.cache_v.to(xq)</span><br><span class="line">   <span class="comment">#防止缓存数据和cache_k和cache_v以及qkv的device不一致</span></span><br><span class="line"></span><br><span class="line">   <span class="variable language_">self</span>.cache_k[:bsz, start_pos : start_pos + seqlen] = xk</span><br><span class="line">   <span class="variable language_">self</span>.cache_v[:bsz, start_pos : start_pos + seqlen] = xv</span><br><span class="line">   <span class="comment">#start pos指的是当前推理的输入的起始位置，seqlen指的是当前推理的输入的长度</span></span><br><span class="line">   <span class="comment">#将转换后的kv存储到kvcache的相应位置中。</span></span><br><span class="line"></span><br><span class="line">   keys = <span class="variable language_">self</span>.cache_k[:bsz, : start_pos + seqlen]</span><br><span class="line">   values = <span class="variable language_">self</span>.cache_v[:bsz, : start_pos + seqlen]</span><br><span class="line">   <span class="comment"># 从cache中取出当前推理的输入的kv</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 如果kv的head头数量小于q的head数量，说明出现了多个qhead共享kv的情况</span></span><br><span class="line">   <span class="comment"># repeat k/v heads if n_kv_heads &lt; n_heads</span></span><br><span class="line">   keys = repeat_kv(keys, <span class="variable language_">self</span>.n_rep)  <span class="comment"># (bs, cache_len + seqlen, n_local_heads, head_dim)</span></span><br><span class="line">   values = repeat_kv(values, <span class="variable language_">self</span>.n_rep)  <span class="comment"># (bs, cache_len + seqlen, n_local_heads, head_dim)</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">##接下来就是正常的attention计算了。</span></span><br><span class="line">   xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line">   keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (bs, n_local_heads, cache_len + seqlen, head_dim)</span></span><br><span class="line">   values = values.transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (bs, n_local_heads, cache_len + seqlen, head_dim)</span></span><br><span class="line">   scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line">   <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">       scores = scores + mask  <span class="comment"># (bs, n_local_heads, seqlen, cache_len + seqlen)</span></span><br><span class="line">   scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">   output = torch.matmul(scores, values)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line">   output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line">   <span class="keyword">return</span> <span class="variable language_">self</span>.wo(output)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] <a target="_blank" rel="noopener" href="https://github.com/meta-llama/llama/blob/main/llama/model.py#L282">llama源码解析</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/kvcache/" data-id="cmdwxao68000r5ox9aniu8aln" data-title="kv-cache-源码解读" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kv-cache/" rel="tag">kv-cache</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-tensorrt-plugin" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/31/tensorrt-plugin/" class="article-date">
  <time class="dt-published" datetime="2025-07-31T03:01:20.000Z" itemprop="datePublished">2025-07-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/31/tensorrt-plugin/">tensorrt-plugin</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/31/tensorrt-plugin/" data-id="cmdwxao5y00045ox9244z5vio" data-title="tensorrt-plugin" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-推理加速/cuda优化基本概念" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/cuda%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" class="article-date">
  <time class="dt-published" datetime="2025-07-31T02:52:51.000Z" itemprop="datePublished">2025-07-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">基础知识</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/cuda%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">resize_cuda</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-基础cuda编程能力"><a href="#1-基础cuda编程能力" class="headerlink" title="1.基础cuda编程能力"></a>1.基础cuda编程能力</h1><table>
<thead>
<tr>
<th align="left">知识点</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">线程索引计算</td>
<td align="left">使用 threadIdx &#x2F; blockIdx &#x2F; blockDim 正确计算每个线程处理的像素位置。必须知道如何从 grid 定位图像中的 x, y。</td>
</tr>
<tr>
<td align="left">全局内存访问</td>
<td align="left">使用 src &#x2F; dst 指针访问图像像素值（如 src[y * width + x]），注意 stride 和通道数（C）影响。</td>
</tr>
<tr>
<td align="left">共享内存 &#x2F; 寄存器</td>
<td align="left">了解何时需要将邻域像素放到共享内存提高访问速度（尤其双线性插值涉及周围 4 个像素）。</td>
</tr>
<tr>
<td align="left">条件边界处理</td>
<td align="left">处理 resize 边界时容易越界，需要特别判断 x, y 合法性。</td>
</tr>
<tr>
<td align="left">数据类型与转换</td>
<td align="left">图像数据常是 uint8，要乘上 scale 转为 float32，还可能涉及归一化（&#x2F;255），再转回 uchar。</td>
</tr>
<tr>
<td align="left">CUDA 核函数定义</td>
<td align="left">正确书写 <strong>global</strong> void resize(…)，并能从 host 侧 launch kernel，传入设备指针和参数。</td>
</tr>
<tr>
<td align="left">CUDA 设备内存管理</td>
<td align="left">使用 cudaMalloc、cudaMemcpy 管理输入输出数据到 GPU。</td>
</tr>
</tbody></table>
<h1 id="2-基本概念"><a href="#2-基本概念" class="headerlink" title="2.基本概念"></a>2.基本概念</h1><ul>
<li>主机：将CPU及系统的内存(内存条)称为主机;</li>
<li>设备：将GPU及GPU本身的显示内存称为设备;</li>
<li>线程(Thread)：一般通过GPU的一个核进行处理;</li>
<li>线程块(Block)：由多个线程组成；各block是并行执行的，block间无法通信，也没有执行顺序。</li>
<li>线程格(Grid)：由多个线程块组成。</li>
<li>核函数(Kernel)：在GPU上执行的函数通常称为核函数;一般通过标识符**<strong>global</strong>**修饰，调用通过&lt;&lt;&lt;参数1,参数2&gt;&gt;&gt;，用于说明内核函数中的线程数量，以及线程是如何组织的。</li>
<li>直观理解一下：1个线程格（grid），里面包含了多个线程块（block），每个线程块里面又包含了多个线程(thread)。线程是最小的单位了。</li>
</ul>
<h1 id="3-threadIdx、blockIdx、blockDim和gridDim"><a href="#3-threadIdx、blockIdx、blockDim和gridDim" class="headerlink" title="3.threadIdx、blockIdx、blockDim和gridDim"></a>3.threadIdx、blockIdx、blockDim和gridDim</h1><p>  <em>我们把线程格和线程块都看作一个三维的矩阵。这里假设线程格是一个3</em>3<em>3的三维矩阵， 线程块是一个4</em>4<em>4的三维矩阵。</em></p>
<h2 id="gridDim"><a href="#gridDim" class="headerlink" title="gridDim"></a>gridDim</h2><p>gridDim.x、gridDim.y、gridDim.z分别表示线程格各个维度的大小。</p>
<ul>
<li>gridDim.x &#x3D; 3：线程格在x轴方向的线程块数量。</li>
<li>gridDim.y &#x3D; 3：线程格在y轴方向的线程块数量。</li>
<li>gridDim.z &#x3D; 3：线程格在z轴方向的线程块数量。</li>
<li>线程格大小为3x3x3，即27个线程块。</li>
</ul>
<h2 id="blockDim"><a href="#blockDim" class="headerlink" title="blockDim"></a>blockDim</h2><p>blockDim.x、blockDim.y、blockDim.z分别表示线程块中各个维度的大小.<br><code>blockDim.x=4   blockDim.y=4  blockDim.z=4</code></p>
<h2 id="blockIdx"><a href="#blockIdx" class="headerlink" title="blockIdx"></a>blockIdx</h2><p>blockIdx.x、blockIdx.y、blockIdx.z分别表示线程块的索引，也就是当前线程块儿在线程格中的位置索引。</p>
<h2 id="threadIdx"><a href="#threadIdx" class="headerlink" title="threadIdx"></a>threadIdx</h2><p>threadIdx.x、threadIdx.y、threadIdx.z分别表示线程的索引，也就是当前线程在线程块儿中的位置索引。</p>
<p>线程格内的总线程个数<em>N</em>为<code>gridDim.x * gridDim.y * gridDim.z * blockDim.x * blockDim.y * blockDim.z</code></p>
<p>那么，当前线程块儿中的线程就可以用索引来找到：</p>
<ul>
<li>首先要找到blockID，<br>$blockId &#x3D; blockIdx.x + blockIdx.y<em>gridDim.x + blockIdx.z</em>gridDim.x<em>gridDim.y;$<br>解释：首先$blockIdx.z</em>gridDim.x<em>gridDim.y$代表的是当前块儿下方所有的block，加上同一层其他行的其他前方block，$blockIdx.y</em>gridDim.x$,最后加上同行前方的block，$blockIdx.x$。</li>
<li>其次要找到threadID，方法和找blockID一样，<br>$threadId &#x3D; threadIdx.x + threadIdx.y<em>blockDim.x + threadIdx.z</em>blockDim.x*blockDim.y;$</li>
<li>然后计算一个线程块儿中有多少个线程，<br>$M &#x3D; blockDim.x<em>blockDim.y</em>blockDim.z$</li>
<li>最后求得当前的线程序列号idx：<br>$idx &#x3D; threadId + M*blockId;$</li>
</ul>
<h1 id="cuda程序的组成"><a href="#cuda程序的组成" class="headerlink" title="cuda程序的组成"></a>cuda程序的组成</h1><ul>
<li><p>一个 CUDA 程序，我们可以把它分成3个部分：</p>
</li>
<li><p>第1部分是： 从主机 (host) 端申请 device memory， <code>cudaMalloc()</code>把要拷贝的内容从 host memory 拷贝到申请的 device memory 里面。</p>
</li>
<li><p>第2部分是： 设备端的核函数对拷贝进来的东西进行计算，来得到和实现运算的结果，图4中的 Kernel 就是指在 GPU 上运行的函数。</p>
</li>
<li><p>第3部分是： 把结果从 device memory 拷贝到申请的 host memory 里面，并且释放设备端的显存和内存。<code>cudaFree()</code> <code>cudaMemcpy( )</code></p>
</li>
</ul>
<h1 id="cuda的内存模型"><a href="#cuda的内存模型" class="headerlink" title="cuda的内存模型"></a>cuda的内存模型</h1><ul>
<li>线程处理器 (SP) 对应线程 (thread)。线程有自己的寄存器和local memory。</li>
<li>多核处理器 (SM) 对应线程块 (thread block)。同一个 block 中的每个 thread 则有共享的一份 share memory。</li>
<li>设备端 (device) 对应线程块组合体 (grid)。所有的 thread (包括不同 block 的 thread) 都共享一份 global memory。不同的grid有自己的global memory。</li>
<li>一个线程块block内的所有线程thread可以协作，他们相互独立，并行。计算完毕之后，可以设置一个时钟进行同步。</li>
<li>一个grid就是block的集合体。</li>
</ul>
<h1 id="add-kernel简单实现"><a href="#add-kernel简单实现" class="headerlink" title="add kernel简单实现"></a>add kernel简单实现</h1><ul>
<li>对比torch和cuda实现的add算子耗时，<code>main.py</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">cuda_module = load(name=<span class="string">&quot;add2&quot;</span>,</span><br><span class="line">                  sources=[<span class="string">&quot;kernel/add2.cpp&quot;</span>, <span class="string">&quot;kernel/add2_kernel.cu&quot;</span>],</span><br><span class="line">                  verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># c = a + b (shape: [n])</span></span><br><span class="line">n = <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">16</span>  <span class="comment"># 64MB per vector × 3 = 192MB</span></span><br><span class="line"><span class="comment"># n = 1024 * 1024</span></span><br><span class="line">a = torch.rand(n, device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">b = torch.rand(n, device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">cuda_c = torch.rand(n, device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">a_np= np.random.rand(n)</span><br><span class="line">b_np= np.random.rand(n)</span><br><span class="line">ntest = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_time</span>(<span class="params">func</span>):</span><br><span class="line">    times = <span class="built_in">list</span>()</span><br><span class="line">    res = <span class="built_in">list</span>()</span><br><span class="line">    <span class="comment"># GPU warm up</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        func()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(ntest):</span><br><span class="line">        <span class="comment"># sync the threads to get accurate cuda running time</span></span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        r = func()</span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        end_time = time.time()</span><br><span class="line"></span><br><span class="line">        times.append((end_time-start_time)*<span class="number">1e6</span>)</span><br><span class="line">        res.append(r)</span><br><span class="line">    <span class="keyword">return</span> times, res</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_cuda</span>():</span><br><span class="line">    cuda_module.torch_launch_add2(cuda_c, a, b, n)</span><br><span class="line">    <span class="keyword">return</span> cuda_c</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_torch</span>():</span><br><span class="line">    <span class="comment"># return None to avoid intermediate GPU memory application</span></span><br><span class="line">    <span class="comment"># for accurate time statistics</span></span><br><span class="line">    <span class="comment"># c = </span></span><br><span class="line">    <span class="comment"># c = a + b</span></span><br><span class="line">    <span class="comment"># print(a)</span></span><br><span class="line">    <span class="keyword">return</span> torch.add(a, b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running cuda...&quot;</span>)</span><br><span class="line">cuda_time, res = show_time(run_cuda)</span><br><span class="line"><span class="comment"># print(res)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cuda time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(cuda_time)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running torch...&quot;</span>)</span><br><span class="line">torch_time, res = show_time(run_torch)</span><br><span class="line"><span class="comment"># print(res)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Torch time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(torch_time)))</span><br></pre></td></tr></table></figure>
结果：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Running cuda...</span><br><span class="line">Cuda time:  137.367us</span><br><span class="line">Running torch...</span><br><span class="line">Torch time:  422.926us</span><br></pre></td></tr></table></figure></li>
<li>cuda add的实现方式 <code>add2_kernel.cu</code></li>
<li><code>int i = threadIdx.x + blockDim.x * blockIdx.x;</code>是用来为每个线程分配一个全局索引 i，让它对应处理输入数组中的一个元素。</li>
<li>CUDA kernel 中的逻辑就是：每个线程执行一次这段代码，并根据 threadIdx + blockIdx 来处理自己的数据片段。</li>
<li>i是当前线程在整个 grid 中的唯一编号（global thread index）。</li>
<li>下边的写法有两种：当输入数据n远远大于线程总数时，每个线程处理的数据不止一个，就需要for循环的方式。还有就是当n&#x3D;线程总数时，不需要写for循环。<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add2_kernel</span><span class="params">(<span class="type">float</span>* c,<span class="type">const</span> <span class="type">float</span>* a,<span class="type">const</span> <span class="type">float</span>* b,<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n; i += gridDim.x * blockDim.x) &#123;</span><br><span class="line">        c[i] = a[i] + b[i];</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add2_kernel</span><span class="params">(<span class="type">float</span>* c,<span class="type">const</span> <span class="type">float</span>* a,<span class="type">const</span> <span class="type">float</span>* b,<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//cuda核内实现的都是一个block的</span></span><br><span class="line">      <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line">      <span class="keyword">if</span>(i &lt; n)&#123;</span><br><span class="line">          c[i] = a[i] + b[i];</span><br><span class="line">          &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_add2</span><span class="params">(<span class="type">float</span>* c,<span class="type">const</span> <span class="type">float</span>* a,<span class="type">const</span> <span class="type">float</span>* b,<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//n 为输入的元素个数</span></span><br><span class="line">    <span class="comment">//线程块大小为1024</span></span><br><span class="line">    <span class="comment">//线程格的数量决定以输入元素的个数和线程块的大小</span></span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">((n + <span class="number">1023</span>) / <span class="number">1024</span>)</span></span>; <span class="comment">//+1023是为了向上取整</span></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    add2_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(c, a, b, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>如果对这部分内容进行规范化整理，就需要添加内存分配和to device，以及运行结束后的内存释放；</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">vecAdd</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//申请内存的大小是 n *sizeof(float)，定义3个指针 A_d，B_d，C_d。</span></span><br><span class="line">    <span class="comment">//cudaMalloc 函数需要传入 1. 指针的指针 (指向申请得到的显存的指针)。2. 申请显存的大小。 所以分别传入 &amp;A_d 和 size。同理后面依次传入 &amp;B_d 和 size，&amp;C_d 和 size。</span></span><br><span class="line">    <span class="comment">//cudaMemcpy 函数需要传入 1. 终点的指针。2. 起点的指针。3. 拷贝的大小。4. 模式。 所以分别传入 A_d, A, size, cudaMemcpyHostToDevice。同理后面依次传入 B_d, B, size, cudaMemcpyHostToDevice 和 C, C_d, size, cudaMemcpyHostToDevice。</span></span><br><span class="line">    <span class="comment">//最后把设备端申请的显存都释放掉。cudaFree 函数需要传入设备端申请显存的指针，即 A_d，B_d，C_d。</span></span><br><span class="line">    <span class="type">int</span> size = n * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="type">float</span>* A_d, *B_d, *C_d;</span><br><span class="line">    <span class="comment">// Transfer A and B to device memory</span></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **) &amp;A_d, size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d, A, size, cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **) &amp;B_d, size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d, B, size, cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="comment">// Allocate device memory for</span></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **) &amp;C_d, size);</span><br><span class="line">    <span class="comment">// Kernel invocation code –to be shown later</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Transfer C from device to host</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C, C_d, size, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="comment">// Free device memory for A, B, C</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(A_d); <span class="built_in">cudaFree</span>(B_d); <span class="built_in">cudaFree</span>(C_d);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>cuda add函数的torch封装 <code>add2.cpp</code>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../include/add2.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">torch_launch_add2</span><span class="params">(torch::Tensor &amp;c,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="type">const</span> torch::Tensor &amp;a,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="type">const</span> torch::Tensor &amp;b,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">launch_add2</span>((<span class="type">float</span> *)c.<span class="built_in">data_ptr</span>(),</span><br><span class="line">                (<span class="type">const</span> <span class="type">float</span> *)a.<span class="built_in">data_ptr</span>(),</span><br><span class="line">                (<span class="type">const</span> <span class="type">float</span> *)b.<span class="built_in">data_ptr</span>(),</span><br><span class="line">                n);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;torch_launch_add2&quot;</span>,</span><br><span class="line">          &amp;torch_launch_add2,</span><br><span class="line">          <span class="string">&quot;add2 kernel warpper&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>cuda add头文件实现<code>add2.h</code><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_add2</span><span class="params">(<span class="type">float</span> *c,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span> *a,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span> *b,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span> n)</span></span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="add-kernel进阶实现"><a href="#add-kernel进阶实现" class="headerlink" title="add kernel进阶实现"></a>add kernel进阶实现</h1><h2 id="1-一次性处理多个元素（Float4-Load-多值处理）"><a href="#1-一次性处理多个元素（Float4-Load-多值处理）" class="headerlink" title="1. 一次性处理多个元素（Float4 Load &#x2F; 多值处理）"></a>1. 一次性处理多个元素（Float4 Load &#x2F; 多值处理）</h2><p>优点：降低全局内存访问次数；更容易达到 memory coalescing（内存合并访问）；带宽利用率更高。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_vectorized</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* A, <span class="type">const</span> <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">  <span class="keyword">for</span> (; i &lt; n; i += stride * <span class="number">4</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (i + <span class="number">3</span> &lt; n) &#123;</span><br><span class="line">          C[i] = A[i] + B[i];</span><br><span class="line">          C[i<span class="number">+1</span>] = A[i<span class="number">+1</span>] + B[i<span class="number">+1</span>];</span><br><span class="line">          C[i<span class="number">+2</span>] = A[i<span class="number">+2</span>] + B[i<span class="number">+2</span>];</span><br><span class="line">          C[i<span class="number">+3</span>] = A[i<span class="number">+3</span>] + B[i<span class="number">+3</span>];</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_vectorized</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* A, <span class="type">const</span> <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    float4* A4 = <span class="built_in">reinterpret_cast</span>&lt;float4*&gt;(A);</span><br><span class="line">    float4* B4 = <span class="built_in">reinterpret_cast</span>&lt;float4*&gt;(B);</span><br><span class="line">    float4* C4 = <span class="built_in">reinterpret_cast</span>&lt;float4*&gt;(C);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">    <span class="keyword">for</span> (; i &lt; n / <span class="number">4</span>; i += stride) &#123;</span><br><span class="line">        float4 a = A4[i];</span><br><span class="line">        float4 b = B4[i];</span><br><span class="line">        C4[i] = <span class="built_in">make_float4</span>(a.x + b.x, a.y + b.y, a.z + b.z, a.w + b.w);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-手动循环展开（4倍展开）"><a href="#2-手动循环展开（4倍展开）" class="headerlink" title="2.手动循环展开（4倍展开）"></a>2.手动循环展开（4倍展开）</h2><p>优点：提升指令集并行，现在每个线程一次完成4次加法。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_unrolled</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = idx; i + stride * <span class="number">3</span> &lt; n; i += stride * <span class="number">4</span>) &#123;</span><br><span class="line">        C[i]              = A[i]              + B[i];</span><br><span class="line">        C[i + stride]     = A[i + stride]     + B[i + stride];</span><br><span class="line">        C[i + stride * <span class="number">2</span>] = A[i + stride * <span class="number">2</span>] + B[i + stride * <span class="number">2</span>];</span><br><span class="line">        C[i + stride * <span class="number">3</span>] = A[i + stride * <span class="number">3</span>] + B[i + stride * <span class="number">3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-share-memory-缓存（此处对于加法意义不大）"><a href="#3-share-memory-缓存（此处对于加法意义不大）" class="headerlink" title="3.share memory 缓存（此处对于加法意义不大）"></a>3.share memory 缓存（此处对于加法意义不大）</h2><p>针对于复杂计算，可以将全局内存的数据提前cp到共享内存中，减少数据搬移的开销</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_shared</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">float</span> shared[];</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* sA = shared;</span><br><span class="line">    <span class="type">float</span>* sB = &amp;shared[blockDim.x];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> gid = blockIdx.x * blockDim.x + tid;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (gid &lt; n) &#123;</span><br><span class="line">        sA[tid] = A[gid];</span><br><span class="line">        sB[tid] = B[gid];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (gid &lt; n) &#123;</span><br><span class="line">        C[gid] = sA[tid] + sB[tid];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>  [1]<a target="_blank" rel="noopener" href="https://www.cnblogs.com/shuimuqingyang/p/15846584.html">cuda核函数逐步拆解</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/cuda%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" data-id="cmdwxao67000o5ox9gnh50zbl" data-title="resize_cuda" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cuda%E5%8A%A0%E9%80%9F/" rel="tag">cuda加速</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-推理加速/resize-cuda" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/resize-cuda/" class="article-date">
  <time class="dt-published" datetime="2025-07-31T02:52:51.000Z" itemprop="datePublished">2025-07-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">推理加速</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/resize-cuda/">resize_cuda</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><strong>本文主要记录一下resize的cuda并行加速的小例子</strong></p>
<h1 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1.基础知识"></a>1.基础知识</h1><p>[1] <a href="../%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%84%9F%E7%9F%A5%E9%A2%84%E5%A4%84%E7%90%86.md">resize原理</a><br>[2] <a href="./cuda%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md">cuda基础</a></p>
<h1 id="2-resize-cpu源码实现"><a href="#2-resize-cpu源码实现" class="headerlink" title="2.resize cpu源码实现"></a>2.resize cpu源码实现</h1><p>由于进行缩放时，每个新像素点的计算方法均一致，故可使用并行计算，opencv中的resize也是这么做的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="3-resize-cuda源码实现"><a href="#3-resize-cuda源码实现" class="headerlink" title="3.resize cuda源码实现"></a>3.resize cuda源码实现</h1><h1 id="4-一致性测试"><a href="#4-一致性测试" class="headerlink" title="4.一致性测试"></a>4.一致性测试</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/resize-cuda/" data-id="cmdwxao69000s5ox96bbf4ne8" data-title="resize_cuda" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cuda%E5%8A%A0%E9%80%9F/" rel="tag">cuda加速</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-基础知识/感知预处理" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/30/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%84%9F%E7%9F%A5%E9%A2%84%E5%A4%84%E7%90%86/" class="article-date">
  <time class="dt-published" datetime="2025-07-30T15:36:01.000Z" itemprop="datePublished">2025-07-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">基础知识</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/30/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%84%9F%E7%9F%A5%E9%A2%84%E5%A4%84%E7%90%86/">img_preproc</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li><em>这部分内容主要讲解感知模型的预处理</em>，待更新：crop，nv12torgb，grid计算…</li>
</ul>
<h1 id="1-resize"><a href="#1-resize" class="headerlink" title="1.resize"></a>1.resize</h1><ul>
<li>中心思想：<br>在目标图像每一个像素点位置，反向映射到原图位置，再通过插值计算像素值。</li>
<li>举个例子：双线性插值（Bilinear Interpolation）<ol>
<li><p>设目标图像中的点 (x’, y’)，反向映射回原图中的浮点坐标为：</p>
<p>   $x &#x3D; \left( x’ + 0.5 \right) \cdot \frac{W_{\text{src}}}{W_{\text{dst}}} - 0.5$<br>   $y &#x3D; \left( y’ + 0.5 \right) \cdot \frac{H_{\text{src}}}{H_{\text{dst}}} - 0.5$<br>   其中的+0.5是为了取像素中心，-0.5是从像素中心回到像素的索引</p>
</li>
<li><p>然后取它周围 4 个整数点（左上、右上、左下、右下）做插值：</p>
<p> $I(x’, y’) &#x3D; w_{00} \cdot I(x_0, y_0) + w_{10} \cdot I(x_1, y_0) + w_{01} \cdot I(x_0, y_1) + w_{11} \cdot I(x_1, y_1)$</p>
<p> 其中,$w_{ij}$ 是双线性插值权重（由距离决定）。</p>
</li>
<li><p>$w_{ij}$的计算方式如下：<br>假设一个浮点坐标 sx &#x3D; 1.75，sy &#x3D; 3.75它在原图中落在：<br>- 左边像素 x &#x3D; 1</p>
<ul>
<li>右边像素 x+1 &#x3D; 2</li>
<li>上边像素 y+1 &#x3D; 4</li>
<li>下边像素 y &#x3D; 3<br>距离左边的距离 u &#x3D; 0.75，距离右边 1-u &#x3D; 0.25，距离上边和下边的距离分别为1-v &#x3D; 0.25和v&#x3D;0.75.那么这个浮点坐标原图中取（x，y）的权重就是（1-u）（1-v）,其他几个可以类比。</li>
</ul>
</li>
</ol>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/30/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%84%9F%E7%9F%A5%E9%A2%84%E5%A4%84%E7%90%86/" data-id="cmdwxao66000m5ox99piufb36" data-title="img_preproc" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/img-preprocess/" rel="tag">img_preprocess</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-基础知识/数据类型详解" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/29/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time class="dt-published" datetime="2025-07-28T16:00:00.000Z" itemprop="datePublished">2025-07-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">基础知识</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/29/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%AF%A6%E8%A7%A3/">vit源码详解(linformer)</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li><em>此篇主要记录数据类型相关的学习知识</em></li>
</ul>
<h1 id="1-浮点数和定点数的概念"><a href="#1-浮点数和定点数的概念" class="headerlink" title="1.浮点数和定点数的概念"></a>1.浮点数和定点数的概念</h1><ul>
<li><p>定点数 &#x3D; 用整数表示的浮点近似，约定小数点的位置不变</p>
</li>
<li><p>假设你有一个小数 f，你想转成定点数 i，只需要：</p>
</li>
<li><p>$\textbf{i} &#x3D; \text{round}(f \times 2^n)$</p>
<p>  其中：<br>  •    f：原始浮点数（比如 0.625）<br>  •    $2^n$：定点精度（小数点后 n 位）<br>  •    i：整数表示的定点值<br>  •    n：你选的定点位数，通常是 8、11、15、16</p>
</li>
</ul>
<h1 id="2-使用定点数计算的原因"><a href="#2-使用定点数计算的原因" class="headerlink" title="2.使用定点数计算的原因"></a>2.使用定点数计算的原因</h1><ul>
<li>GPU（尤其 OpenCL 的嵌入式后端，如 Adreno、Mali）上：<ul>
<li>int 运算性能 &gt;&gt; float 运算</li>
<li>定点数精度足够 + 更快</li>
<li>减少功耗，适合移动端部署</li>
<li>OpenCL、嵌入式 GPU 通常选择 11~15 位</li>
</ul>
</li>
</ul>
<h1 id="3-例子"><a href="#3-例子" class="headerlink" title="3.例子"></a>3.例子</h1><ul>
<li>代码如下：  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> u = <span class="number">0.625</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定点位数：11位</span></span><br><span class="line"><span class="type">int</span> fixed_u = <span class="built_in">round</span>(u * (<span class="number">1</span> &lt;&lt; <span class="number">11</span>));  <span class="comment">// 0.625 * 2048 = 1280</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 恢复成浮点：</span></span><br><span class="line"><span class="type">float</span> u_recovered = fixed_u / <span class="built_in">float</span>(<span class="number">1</span> &lt;&lt; <span class="number">11</span>); <span class="comment">// = 0.625</span></span><br></pre></td></tr></table></figure></li>
<li>opencl<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">int U = rint(u * (1 &lt;&lt; 11));</span><br><span class="line">int V = rint(v * (1 &lt;&lt; 11));</span><br><span class="line">int U1 = rint((1 &lt;&lt; 11) - U);</span><br><span class="line">int V1 = rint((1 &lt;&lt; 11) - V);</span><br><span class="line"></span><br><span class="line">// 模拟浮点乘法权重：</span><br><span class="line">int result = mul24(U1, V1) * data0</span><br><span class="line">        + mul24(U, V1) * data1</span><br><span class="line">        + mul24(U1, V) * data2</span><br><span class="line">        + mul24(U, V) * data3;</span><br><span class="line"></span><br><span class="line">// 最后右移 22（因为两个 11 位乘了两次）取出有效部分</span><br><span class="line">uchar val = (result + (1 &lt;&lt; 21)) &gt;&gt; 22;</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/29/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%AF%A6%E8%A7%A3/" data-id="cmdwxao66000n5ox972x5bw41" data-title="vit源码详解(linformer)" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/float/" rel="tag">float</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/int/" rel="tag">int</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%9A%E7%82%B9%E6%95%B0/" rel="tag">定点数</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B5%AE%E7%82%B9%E6%95%B0/" rel="tag">浮点数</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-推理加速/vit-profiling" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/28/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/vit-profiling/" class="article-date">
  <time class="dt-published" datetime="2025-07-28T06:08:36.000Z" itemprop="datePublished">2025-07-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">推理加速</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/28/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/vit-profiling/">vit-torch-profiling</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1.前置知识"></a>1.前置知识</h1><ul>
<li><a href="../%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/flops%E5%92%8C%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6.md">flops和算法复杂度</a></li>
</ul>
<h1 id="2-torch-profiler"><a href="#2-torch-profiler" class="headerlink" title="2.torch.profiler"></a>2.torch.profiler</h1><ul>
<li>代码如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vit_profiler.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.profiler <span class="keyword">import</span> profile, record_function, ProfilerActivity</span><br><span class="line"><span class="keyword">from</span> vit_pytorch.efficient <span class="keyword">import</span> ViT  </span><br><span class="line"><span class="keyword">from</span> linformer <span class="keyword">import</span> Linformer</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">efficient_transformer = Linformer(</span><br><span class="line">    dim=<span class="number">128</span>,</span><br><span class="line">    seq_len=<span class="number">49</span>+<span class="number">1</span>,  <span class="comment"># 7x7 patches + 1 cls-token</span></span><br><span class="line">    depth=<span class="number">12</span>,</span><br><span class="line">    heads=<span class="number">8</span>,</span><br><span class="line">    k=<span class="number">64</span></span><br><span class="line">)</span><br><span class="line">model = ViT(</span><br><span class="line">    dim=<span class="number">128</span>,</span><br><span class="line">    image_size=<span class="number">224</span>,</span><br><span class="line">    patch_size=<span class="number">32</span>,</span><br><span class="line">    num_classes=<span class="number">2</span>,</span><br><span class="line">    transformer=efficient_transformer,</span><br><span class="line">    channels=<span class="number">3</span>,</span><br><span class="line">).to(device)</span><br><span class="line">model = nn.DataParallel(model)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&quot;model/best_model_vit1.pth&quot;</span>, map_location=<span class="string">&quot;cpu&quot;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">dummy_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> profile(</span><br><span class="line">    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],</span><br><span class="line">    record_shapes=<span class="literal">True</span>,</span><br><span class="line">    with_flops=<span class="literal">True</span>,</span><br><span class="line">    profile_memory=<span class="literal">True</span>,</span><br><span class="line">    on_trace_ready=torch.profiler.tensorboard_trace_handler(<span class="string">&quot;profiling/vit_trace&quot;</span>)</span><br><span class="line">) <span class="keyword">as</span> prof:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">with</span> record_function(<span class="string">&quot;vit_inference&quot;</span>):</span><br><span class="line">            model(dummy_input)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;profiling/vit_profile_summary.md&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(prof.key_averages().table(sort_by=<span class="string">&quot;cuda_time_total&quot;</span>, row_limit=<span class="number">30</span>))</span><br></pre></td></tr></table></figure></li>
<li>运行结果使用tensorboard查看：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=profiling/vit_trace</span><br></pre></td></tr></table></figure></li>
</ul>
<ol start="3">
<li>flops统计<br>  在 Transformer 或 ViT 等模型中，隐藏维度（如 embedding dim） 决定了绝大多数算子的计算量，例如：<ul>
<li>Linear: FLOPs -&gt; 输入维度 × 输出维度</li>
<li>MatMul: FLOPs -&gt; hidden_dim² × 序列长度</li>
<li>FFN: 通常是 hidden_dim → 4×hidden_dim → hidden_dim，两个linear层，FLOPs 非常大</li>
</ul>
</li>
</ol>
<ul>
<li>hidden_dim 增加 6 倍后，FLOPs 增加 hidden_dim² 倍</li>
<li>测试代码如下：</li>
</ul>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##hidden_dim = 768</span></span><br><span class="line"><span class="keyword">from</span> ptflops <span class="keyword">import</span> get_model_complexity_info</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.profiler <span class="keyword">import</span> profile, record_function, ProfilerActivity</span><br><span class="line"><span class="keyword">from</span> vit_pytorch.efficient <span class="keyword">import</span> ViT <span class="keyword">as</span> ViT_linformer</span><br><span class="line"><span class="keyword">from</span> linformer <span class="keyword">import</span> Linformer</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> vit_pytorch <span class="keyword">import</span> ViT</span><br><span class="line"></span><br><span class="line">efficient_transformer = Linformer(</span><br><span class="line">    dim=<span class="number">768</span>,</span><br><span class="line">    seq_len=<span class="number">49</span>+<span class="number">1</span>,  <span class="comment"># 7x7 patches + 1 cls-token</span></span><br><span class="line">    depth=<span class="number">12</span>,</span><br><span class="line">    heads=<span class="number">8</span>,</span><br><span class="line">    k=<span class="number">20</span></span><br><span class="line">)</span><br><span class="line">models = &#123;</span><br><span class="line">    <span class="string">&quot;linformer&quot;</span>: ViT_linformer(dim=<span class="number">768</span>,</span><br><span class="line">        image_size=<span class="number">224</span>,</span><br><span class="line">        patch_size=<span class="number">32</span>,</span><br><span class="line">        num_classes=<span class="number">2</span>,</span><br><span class="line">        transformer=efficient_transformer,</span><br><span class="line">        channels=<span class="number">3</span>),</span><br><span class="line">    <span class="string">&quot;mha&quot;</span>: ViT( dim=<span class="number">768</span>,</span><br><span class="line">        image_size=<span class="number">224</span>,</span><br><span class="line">        patch_size=<span class="number">32</span>,</span><br><span class="line">        num_classes=<span class="number">2</span>,</span><br><span class="line">        depth=<span class="number">12</span>, heads=<span class="number">8</span>, mlp_dim= <span class="number">768</span>*<span class="number">4</span>,</span><br><span class="line">        )</span><br><span class="line">&#125;</span><br><span class="line">macs, params = get_model_complexity_info(models[<span class="string">&#x27;mha&#x27;</span>], (<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;FLOPs: <span class="subst">&#123;macs * <span class="number">2</span>&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment">#macs, params = get_model_complexity_info(models[&#x27;linformer&#x27;], (3, 224, 224))</span></span><br><span class="line"><span class="comment">#print(f&quot;FLOPs: &#123;macs * 2&#125;&quot;)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>运行结果如下:</li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">ViT(</span><br><span class="line">  77.96 M, 99.950% Params, 3.9 GMac, 99.165% MACs, </span><br><span class="line">  (to_patch_embedding): Sequential(</span><br><span class="line">    2.37 M, 3.036% Params, 115.83 MMac, 2.948% MACs, </span><br><span class="line">    (0): Rearrange(&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;, p1=32, p2=32)</span><br><span class="line">    (1): LayerNorm(6.14 k, 0.008% Params, 150.53 KMac, 0.004% MACs, (3072,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">    (2): Linear(2.36 M, 3.026% Params, 115.64 MMac, 2.943% MACs, in_features=3072, out_features=768, bias=True)</span><br><span class="line">    (3): LayerNorm(1.54 k, 0.002% Params, 37.63 KMac, 0.001% MACs, (768,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)</span><br><span class="line">  (transformer): Transformer(</span><br><span class="line">    75.59 M, 96.912% Params, 3.78 GMac, 96.217% MACs, </span><br><span class="line">    (norm): LayerNorm(1.54 k, 0.002% Params, 38.4 KMac, 0.001% MACs, (768,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (0-11): 12 x ModuleList(</span><br><span class="line">        (0): Attention(</span><br><span class="line">          1.58 M, 2.019% Params, 78.72 MMac, 2.004% MACs, </span><br><span class="line">          (norm): LayerNorm(1.54 k, 0.002% Params, 38.4 KMac, 0.001% MACs, (768,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">          (attend): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)</span><br><span class="line">          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)</span><br><span class="line">          (to_qkv): Linear(1.18 M, 1.512% Params, 58.98 MMac, 1.501% MACs, in_features=768, out_features=1536, bias=False)</span><br><span class="line">          (to_out): Sequential(</span><br><span class="line">            393.98 k, 0.505% Params, 19.7 MMac, 0.501% MACs, </span><br><span class="line">            (0): Linear(393.98 k, 0.505% Params, 19.7 MMac, 0.501% MACs, in_features=512, out_features=768, bias=True)</span><br><span class="line">            (1): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): FeedForward(</span><br><span class="line">          4.72 M, 6.056% Params, 236.31 MMac, 6.014% MACs, </span><br><span class="line">          (net): Sequential(</span><br><span class="line">            4.72 M, 6.056% Params, 236.31 MMac, 6.014% MACs, </span><br><span class="line">            (0): LayerNorm(1.54 k, 0.002% Params, 38.4 KMac, 0.001% MACs, (768,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">            (1): Linear(2.36 M, 3.029% Params, 118.12 MMac, 3.006% MACs, in_features=768, out_features=3072, bias=True)</span><br><span class="line">            (2): GELU(0, 0.000% Params, 153.6 KMac, 0.004% MACs, approximate=&#x27;none&#x27;)</span><br><span class="line">            (3): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)</span><br><span class="line">            (4): Linear(2.36 M, 3.026% Params, 118.0 MMac, 3.003% MACs, in_features=3072, out_features=768, bias=True)</span><br><span class="line">            (5): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (to_latent): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )</span><br><span class="line">  (mlp_head): Linear(1.54 k, 0.002% Params, 1.54 KMac, 0.000% MACs, in_features=768, out_features=2, bias=True)</span><br><span class="line">)</span><br><span class="line">FLOPs: 3.93 GMac</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/28/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/vit-profiling/" data-id="cmdwxao6a000w5ox9ctv6flbs" data-title="vit-torch-profiling" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/profiling/" rel="tag">profiling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vit/" rel="tag">vit</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vit-pytorch/" rel="tag">vit-pytorch</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-周计划/Week5" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week5/" class="article-date">
  <time class="dt-published" datetime="2025-07-28T05:16:38.000Z" itemprop="datePublished">2025-07-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week5/">Week5</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week5/" data-id="cmdwxao63000e5ox93jk6bq5l" data-title="Week5" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-周计划/Week4" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week4/" class="article-date">
  <time class="dt-published" datetime="2025-07-28T05:16:35.000Z" itemprop="datePublished">2025-07-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week4/">Week4</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week4/" data-id="cmdwxao63000d5ox96ho42iqi" data-title="Week4" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-周计划/Week3" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week3/" class="article-date">
  <time class="dt-published" datetime="2025-07-28T05:16:32.000Z" itemprop="datePublished">2025-07-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week3/">Week3</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/28/%E5%91%A8%E8%AE%A1%E5%88%92/Week3/" data-id="cmdwxao6200095ox96u1860ip" data-title="Week3" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%91%A8%E8%AE%A1%E5%88%92/">周计划</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">基础知识</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">推理加速</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/FLOPS/" rel="tag">FLOPS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/attention/" rel="tag">attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/conv/" rel="tag">conv</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cuda%E5%8A%A0%E9%80%9F/" rel="tag">cuda加速</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/float/" rel="tag">float</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/img-preprocess/" rel="tag">img_preprocess</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/int/" rel="tag">int</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kv-cache/" rel="tag">kv-cache</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linformer/" rel="tag">linformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/profiling/" rel="tag">profiling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vit/" rel="tag">vit</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vit-pytorch/" rel="tag">vit-pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/week1/" rel="tag">week1</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9A%E7%82%B9%E6%95%B0/" rel="tag">定点数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/" rel="tag">小技巧</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/" rel="tag">显存占用</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%AE%E7%82%B9%E6%95%B0/" rel="tag">浮点数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/" rel="tag">算法复杂度</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%90%E7%AE%97%E9%87%8F/" rel="tag">运算量</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/FLOPS/" style="font-size: 10px;">FLOPS</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/attention/" style="font-size: 20px;">attention</a> <a href="/tags/conv/" style="font-size: 10px;">conv</a> <a href="/tags/cuda%E5%8A%A0%E9%80%9F/" style="font-size: 20px;">cuda加速</a> <a href="/tags/float/" style="font-size: 10px;">float</a> <a href="/tags/img-preprocess/" style="font-size: 10px;">img_preprocess</a> <a href="/tags/int/" style="font-size: 10px;">int</a> <a href="/tags/kv-cache/" style="font-size: 10px;">kv-cache</a> <a href="/tags/linformer/" style="font-size: 10px;">linformer</a> <a href="/tags/profiling/" style="font-size: 10px;">profiling</a> <a href="/tags/vit/" style="font-size: 20px;">vit</a> <a href="/tags/vit-pytorch/" style="font-size: 20px;">vit-pytorch</a> <a href="/tags/week1/" style="font-size: 10px;">week1</a> <a href="/tags/%E5%AE%9A%E7%82%B9%E6%95%B0/" style="font-size: 10px;">定点数</a> <a href="/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/" style="font-size: 10px;">小技巧</a> <a href="/tags/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/" style="font-size: 10px;">显存占用</a> <a href="/tags/%E6%B5%AE%E7%82%B9%E6%95%B0/" style="font-size: 10px;">浮点数</a> <a href="/tags/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/" style="font-size: 10px;">算法复杂度</a> <a href="/tags/%E8%BF%90%E7%AE%97%E9%87%8F/" style="font-size: 10px;">运算量</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/kvcache/">kv-cache-源码解读</a>
          </li>
        
          <li>
            <a href="/2025/07/31/tensorrt-plugin/">tensorrt-plugin</a>
          </li>
        
          <li>
            <a href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/cuda%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">resize_cuda</a>
          </li>
        
          <li>
            <a href="/2025/07/31/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/resize-cuda/">resize_cuda</a>
          </li>
        
          <li>
            <a href="/2025/07/30/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%84%9F%E7%9F%A5%E9%A2%84%E5%A4%84%E7%90%86/">img_preproc</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 sunhanyu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>